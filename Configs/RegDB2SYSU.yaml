
xx_port: 0.9                     # The portion of the single domain batch (src-src/target-taget, or AA/BB)
ab_port: 0.1
max_round: 40
adv_warm_max_round: 1
lr2_ramp_factor: 60
aa_drop:    True
lr2: 0.00001  
data_root_a:    /home/tianyu/code/SamWorkSpace/HiCMD-master/data/RegDB_01
data_root_b:    /home/tianyu/code/SamWorkSpace/HiCMD-master/data/SYSU
OuputModelDir:  /home/tianyu/code/SamWorkSpace/HiCMD-master/model/RegDB2SYSU    
batch_size: 1
epoch_round_adv: 1 #22             # 产生一次伪标签后，数据集合训练的次数。
ID_class_a: 206                  # The number of ID classes of source domain. Eg:RegDB:206
ID_class_b: 0
id_tgt: false
log_iter: 100                      # How often do you want to log the training stats
id_adv_w: 1.000                  # The initial weight of domain ID adversarial loss
aa: true                         # Whether to use the AA-type batch
ab: true                         # Whether to use the AB-type batch
bb: true                         # Whether to use the BB-type batch
aa_drop: true                    # Whether to drop the aa batch in self-training
epoch_round: 2
gen:
  activ: lrelu                   # activation function style [relu/lrelu/prelu/selu/tanh]
  dec: basic                     # [basic/parallel/series]
  dim: 16                        # number of filters in the bottommost layer
  dropout: 0                     # use dropout in the generator
  id_dim: 2048                   # length of appearance code
  mlp_dim: 512                   # number of filters in MLP
  mlp_norm: none                 # norm in mlp [none/bn/in/ln]
  n_downsample: 2                # number of downsampling layers in content encoder
  n_res: 4                       # number of residual blocks in content encoder/decoder
  non_local: 0                   # number of non_local layer
  pad_type: reflect              # padding type [zero/reflect]
  tanh: false                    # use tanh or not at the last layer
  init: kaiming                  # initialization [gaussian/kaiming/xavier/orthogonal]

dis:
  # for image discriminator
  LAMBDA: 0.01                   # the hyperparameter for the regularization term
  activ: lrelu                   # activation function style [relu/lrelu/prelu/selu/tanh]
  dim: 32                        # number of filters in the bottommost layer
  gan_type: lsgan                # GAN loss [lsgan/nsgan]
  n_layer: 2                     # number of layers in D
  n_res: 4                       # number of layers in D
  non_local: 0                   # number of non_local layers
  norm: none                     # normalization layer [none/bn/in/ln]
  num_scales: 3                  # number of scales
  pad_type: reflect              # padding type [zero/reflect]
  # for domain id discriminator
  id_ganType: lsgan              # the type of the network of ID discriminator
  id_activ: lrelu                # activation function style [relu/lrelu/prelu/selu/tanh]
  id_norm: bn                    # normalization layer [none/bn/in/ln]
  id_nLayer: 4                   # number of layers in domain id discriminator
  id_nFilter: 1024               # number of layer filters in domain id discriminator
  id_ds: 2                       # down sampling rate in domain id discriminator